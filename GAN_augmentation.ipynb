{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VeN7RrpvlKgX"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import (Dense, \n",
        "                                     BatchNormalization, \n",
        "                                     LeakyReLU, \n",
        "                                     Reshape, \n",
        "                                     Conv2DTranspose,\n",
        "                                     Conv2D,\n",
        "                                     Dropout,\n",
        "                                     Flatten)\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from skimage.io import imread\n",
        "from skimage.color import rgb2lab, lab2rgb\n",
        "import matplotlib.pylab as plt\n",
        "from google.colab.patches import cv2_imshow\n",
        "from google.colab import files\n",
        "\n",
        "NUM_IMAGES = 128 #Number of images\n",
        "\n",
        "all_imgs = np.zeros((28, 28, NUM_IMAGES))\n",
        "\n",
        "def read_file(filename):\n",
        "  image = cv2.imread(filename)\n",
        "  cv2_imshow(image)\n",
        "  return image\n",
        "\n",
        "for x in range(NUM_IMAGES):\n",
        "  uploaded = files.upload()\n",
        "  filename = next(iter(uploaded))\n",
        "  image = read_file(filename)\n",
        "  image1 = rgb2lab(image)\n",
        "\n",
        "  image1[...,1] = image1[...,2] = 0\n",
        "\n",
        "  image1 = lab2rgb(image1)\n",
        "\n",
        "  img = np.float32(image1[:, :, 0])\n",
        "\n",
        "  all_imgs[:, :, x] = img\n",
        "\n"
      ],
      "metadata": {
        "id": "DJMueVOa3weW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(all_imgs.shape)\n",
        "#print(all_imgs)\n",
        "\n",
        "\n",
        "new_train_images = img.reshape(1, 28, 28, 1).astype('float32')\n",
        "new_train_images = (new_train_images - 127.5) / 127.5 # Normalize the images to [-1, 1]\n",
        "\n",
        "print(new_train_images.shape)\n"
      ],
      "metadata": {
        "id": "NBf10VnV-Wl1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_images = new_train_images\n",
        "\n",
        "BUFFER_SIZE = 60000\n",
        "BATCH_SIZE = 256\n",
        "\n",
        "# Batch and shuffle the data\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n"
      ],
      "metadata": {
        "id": "fyXiw5hVlxwX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_generator_model():\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(Dense(7*7*256, use_bias=False, input_shape=(100,)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(LeakyReLU())\n",
        "\n",
        "    model.add(Reshape((7, 7, 256)))\n",
        "    assert model.output_shape == (None, 7, 7, 256) # Note: None is the batch size\n",
        "\n",
        "    model.add(Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))\n",
        "    assert model.output_shape == (None, 7, 7, 128)\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(LeakyReLU())\n",
        "\n",
        "    model.add(Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
        "    assert model.output_shape == (None, 14, 14, 64)\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(LeakyReLU())\n",
        "\n",
        "    model.add(Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))\n",
        "    assert model.output_shape == (None, 28, 28, 1)\n",
        "    #assert model.output_shape == (None, 1920, 1080, 3)\n",
        "\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "qCFmcvtYl3Gk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator = make_generator_model()"
      ],
      "metadata": {
        "id": "HeAwpCEhl6GL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a random noise and generate a sample\n",
        "noise = tf.random.normal([1, 100])\n",
        "generated_image = generator(noise, training=False)\n",
        "# Visualize the generated sample\n",
        "plt.imshow(generated_image[0, :, :, 0], cmap='gray')"
      ],
      "metadata": {
        "id": "Yx45BWzdl_IM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_discriminator_model():\n",
        "    model = tf.keras.Sequential()\n",
        "    \n",
        "    model.add(Conv2D(64, (5, 5), strides=(2, 2), padding='same', input_shape=[28, 28, 1]))\n",
        "\n",
        "    #model.add(Conv2D(64, (5, 5), strides=(2, 2), padding='same', input_shape=[1080, 1920, 3]))\n",
        "    model.add(LeakyReLU())\n",
        "    model.add(Dropout(0.3))\n",
        "\n",
        "    model.add(Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n",
        "    model.add(LeakyReLU())\n",
        "    model.add(Dropout(0.3))\n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(1))\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "XPtkteKomKqq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "discriminator = make_discriminator_model()"
      ],
      "metadata": {
        "id": "qUQzg7ZemNYo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decision = discriminator(generated_image)\n",
        "print (decision)"
      ],
      "metadata": {
        "id": "nvG1OlO1mOqG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This method returns a helper function to compute cross entropy loss\n",
        "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "def discriminator_loss(real_output, fake_output):\n",
        "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
        "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
        "    total_loss = real_loss + fake_loss\n",
        "    return total_loss\n",
        "\n",
        "def generator_loss(fake_output):\n",
        "    return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
        "\n",
        "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)"
      ],
      "metadata": {
        "id": "dpHZeGcVmXlP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
        "                                 discriminator_optimizer=discriminator_optimizer,\n",
        "                                 generator=generator,\n",
        "                                 discriminator=discriminator)"
      ],
      "metadata": {
        "id": "92jTkB0uma6m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 6000\n",
        "# We will reuse this seed overtime (so it's easier)\n",
        "# to visualize progress in the animated GIF)\n",
        "num_examples_to_generate = 16\n",
        "noise_dim = 100\n",
        "seed = tf.random.normal([num_examples_to_generate, noise_dim])"
      ],
      "metadata": {
        "id": "CVhD1w6Wmb-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tf.function annotation causes the function \n",
        "# to be \"compiled\" as part of the training\n",
        "@tf.function\n",
        "def train_step(images):\n",
        "  \n",
        "    # 1 - Create a random noise to feed it into the model\n",
        "    # for the image generation\n",
        "    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n",
        "    \n",
        "    # 2 - Generate images and calculate loss values\n",
        "    # GradientTape method records operations for automatic differentiation.\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "      generated_images = generator(noise, training=True)\n",
        "\n",
        "      real_output = discriminator(images, training=True)\n",
        "      fake_output = discriminator(generated_images, training=True)\n",
        "\n",
        "      gen_loss = generator_loss(fake_output)\n",
        "      disc_loss = discriminator_loss(real_output, fake_output)\n",
        "\n",
        "    # 3 - Calculate gradients using loss values and model variables\n",
        "    # \"gradient\" method computes the gradient using \n",
        "    # operations recorded in context of this tape (gen_tape and disc_tape).\n",
        "    \n",
        "    # It accepts a target (e.g., gen_loss) variable and \n",
        "    # a source variable (e.g.,generator.trainable_variables)\n",
        "    # target --> a list or nested structure of Tensors or Variables to be differentiated.\n",
        "    # source --> a list or nested structure of Tensors or Variables.\n",
        "    # target will be differentiated against elements in sources.\n",
        "\n",
        "    # \"gradient\" method returns a list or nested structure of Tensors  \n",
        "    # (or IndexedSlices, or None), one for each element in sources. \n",
        "    # Returned structure is the same as the structure of sources.\n",
        "    gradients_of_generator = gen_tape.gradient(gen_loss, \n",
        "                                               generator.trainable_variables)\n",
        "    gradients_of_discriminator = disc_tape.gradient(disc_loss, \n",
        "                                                discriminator.trainable_variables)\n",
        "    \n",
        "    # 4 - Process  Gradients and Run the Optimizer\n",
        "    # \"apply_gradients\" method processes aggregated gradients. \n",
        "    # ex: optimizer.apply_gradients(zip(grads, vars))\n",
        "    \"\"\"\n",
        "    Example use of apply_gradients:\n",
        "    grads = tape.gradient(loss, vars)\n",
        "    grads = tf.distribute.get_replica_context().all_reduce('sum', grads)\n",
        "    # Processing aggregated gradients.\n",
        "    optimizer.apply_gradients(zip(grads, vars), experimental_aggregate_gradients=False)\n",
        "    \"\"\"\n",
        "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
        "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n"
      ],
      "metadata": {
        "id": "Jiuv4Af9mjdg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from IPython import display # A command shell for interactive computing in Python.\n",
        "\n",
        "def train(dataset, epochs):\n",
        "  # A. For each epoch, do the following:\n",
        "  for epoch in range(epochs):\n",
        "    start = time.time()\n",
        "    # 1 - For each batch of the epoch, \n",
        "    for image_batch in dataset:\n",
        "      # 1.a - run the custom \"train_step\" function\n",
        "      # we just declared above\n",
        "      train_step(image_batch)\n",
        "\n",
        "    # 2 - Produce images for the GIF as we go\n",
        "    display.clear_output(wait=True)\n",
        "    generate_and_save_images(generator,\n",
        "                             epoch + 1,\n",
        "                             seed)\n",
        "\n",
        "    # 3 - Save the model every 5 epochs as \n",
        "    # a checkpoint, which we will use later\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "      checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "    # 4 - Print out the completed epoch no. and the time spent\n",
        "    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
        "\n",
        "  # B. Generate a final image after the training is completed\n",
        "  display.clear_output(wait=True)\n",
        "  generate_and_save_images(generator,\n",
        "                           epochs,\n",
        "                           seed)"
      ],
      "metadata": {
        "id": "et0TVwpLml37"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_and_save_images(model, epoch, test_input):\n",
        "  # Notice `training` is set to False.\n",
        "  # This is so all layers run in inference mode (batchnorm).\n",
        "  # 1 - Generate images\n",
        "  predictions = model(test_input, training=False)\n",
        "  # 2 - Plot the generated images\n",
        "  fig = plt.figure(figsize=(4,4))\n",
        "  for i in range(predictions.shape[0]):\n",
        "      plt.subplot(4, 4, i+1)\n",
        "      plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
        "      plt.axis('off')\n",
        "  # 3 - Save the generated images\n",
        "  plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "Vn17ggsXmqol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(train_dataset, EPOCHS)"
      ],
      "metadata": {
        "id": "ZuDqWJ2OmtAQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ],
      "metadata": {
        "id": "1zLUgwxNrcvH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PIL is a library which may open different image file formats\n",
        "import PIL \n",
        "# Display a single image using the epoch number\n",
        "def display_image(epoch_no):\n",
        "  return PIL.Image.open('image_at_epoch_{:04d}.png'.format(epoch_no))\n",
        "display_image(EPOCHS)"
      ],
      "metadata": {
        "id": "cMLTQctOrieC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import glob # The glob module is used for Unix style pathname pattern expansion.\n",
        "import imageio # The library that provides an easy interface to read and write a wide range of image data\n",
        "\n",
        "anim_file = 'dcgan.gif'\n",
        "\n",
        "with imageio.get_writer(anim_file, mode='I') as writer:\n",
        "  filenames = glob.glob('image*.png')\n",
        "  filenames = sorted(filenames)\n",
        "  for filename in filenames:\n",
        "    image = imageio.imread(filename)\n",
        "    writer.append_data(image)\n",
        "  # image = imageio.imread(filename)\n",
        "  # writer.append_data(image)\n",
        "  \n",
        "display.Image(open('dcgan.gif','rb').read())"
      ],
      "metadata": {
        "id": "67rAX3w_rmcw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}